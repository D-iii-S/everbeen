<?xml version="1.0" encoding="UTF-8"?>

<sect1
	xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xml="http://www.w3.org/XML/1998/namespace"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xsi:schemaLocation="http://docbook.org/ns/docbook ../../../docbook-5.0/xsd/docbook.xsd"
>
	<title>Benchmark Manager and Analyses</title>

	<para>
	Benchmarking any software is mostly statistics. Ussually we want to either compare two products or see some performance changes in a single program over time. Been can be used for both but it is especially usefull in the second type of tasks. In the following sections we will focus on potentially endless regression testing. This type of analysis measures different versions of a software and compares the measured data between versions. Comparison analyses can be performed by running the same benchmark on two or more products.
	</para>


	<sect2>
		<title>Analysis</title>

		<para>
		A compact measurement set is labeled as an analysis. In regression benchmarking, an analysis is ussually divided into experiments. Experiment ussually runs the benchmark and produces measured data. For each source file version we want several binaries and for each binary we want some runs. BEEN is not limited by this structure but we will reffer to this use case because it is the ussual scenario.
		</para>

		<para>
		Every experiment needs two steps to generate some results. We need to generate data and then evaluate it.
		</para>

		<sect3>
			<title>Generating Data</title>

			<sect4>
				<title>Benchmarking software</title>

				<para>
				Benchmarks are very project-specific. You will need to provide your own measuring software which generates the data. Under some circumstances even unit tests may be used to measure the software performance although it is not generally recomended. BEEN is a distributed framework to support even distributed benchmarks.
				</para>

				<para>
				The measuring software has to be included in or runable by a BEEN task. BEEN supports Jython and shell scripting. In practice you can either create new task package with the script or use existing scripting task to run it. The same applies on the measured software.
				</para>

			</sect4>

			<sect4>
				<title>Download, Build, Run tasks</title>

				<para>
				BEEN can download, build, and run any piece of software. It only needs the task packages that are able to perform the requested actions. Some common actions are supported by the packages included in the default distribution like downloading from CVS or SVN repository.
				</para>

				<para>
				Building and running the tested and the measuring software is not supported in any way. The software usualy neds specific deployment and enviroment settings. This can be easily iplemented using shell script tasks.
				</para>
			</sect4>

			<sect4>
				<title>Generator</title>

				<para>
				The generator pluggable module is usually specific for a benchmark. It has to know what tasks are necessary in the download, buid, run sequence and what are their dependencies. The generator is also responsible for deciding what should be done. In another words it should know version number to download, whether to compile new build, how many runs to perform on which build.
				</para>

				<para>
				Generator is also responsible for creating result datasets in the repository. More about datasets in the next section
				</para>

			</sect4>

			<sect4>
				<title>Collecting Results</title>

				<para>
				Usually the task that runs the benchmark also collects data and sends it to the Results Repository. Alternativelly the data collection can be separated. Either way the communication with the Results Repository has to be done from Java task. BEEN provides some pluggable modules that make creating of such a task easier.
				</para>

			</sect4>

		</sect3>

		<sect3>
			<title>Evaluating Data</title>

			<para>
			Each analysis in BEEN can have some evaluators. Unlike generators, evaluators are not tied to specific benchmark but rather to a specific data format. If there are two different benchmarks with the same data structure (in the Results Repository) then we can use the same evaluator to evaluate both.
			</para>

			<para>
			Evaluator pluggable modules are responsible for creating triggers in the Results Repositoy. In short these triggers schedule an evaluator task whenever new data arive. There is a whole section dedicated to triggers, read it to get more information.
			</para>

			<para>
			Evaluator task gets the information about the arived data, reads the appropriate part of the dataset from the Results Repository and performs the calculation. When done, it can either write the result back to the Results Repository or somewhere to the filestore. Writing back to the Results Repository may tigger another task.
			</para>

			<para>
			Default BEEN distribution offers generic task that can download data from the Results Repository, run the statistical software R on the data and then save the results back. Another task can download files from the Results Repository to a local directory.
			</para>

		</sect3>

		<sect3>
			<title>Review</title>

			<para>
			The following list shows what you need to run your analysis. Note that some parts of this list may be already included in the default BEEN distribution. For example there are SVN download task, script runner task, generic R evaluator task and pluggable module. Also some parts of the list may of course not apply to your analysis.
			</para>

			<itemizedlist>
				<title>BEEN Analysis checklist</title>
				<listitem>
					<para>The tested software - you need to know what you measure and where to get it</para>
				</listitem>
				<listitem>
					<para>The benchmarking software - you need to have the testing tool</para>
				</listitem>
				<listitem>
					<para>Tasks that download, build and run the tested and the benchmarking sofware</para>
				</listitem>
				<listitem>
					<para>Generator pluggable module</para>
				</listitem>
				<listitem>
					<para>Evaluator task(s)</para>
				</listitem>
				<listitem>
					<para>Evaluator pluggable module</para>
				</listitem>
			</itemizedlist>

		</sect3>

	</sect2>

	<sect2>
		<title>Benchmark Manager</title>

		<para>
		Benchmark Manager is a BEEN service responsible for managing analyses and executing experiments.
		</para>

		<para>
		In regression benchmarking there are many measurements that need to be performed. Generator pluggable module may schedule a lot of work at once but this approach is not very transparent. It is better to let the generator schedule only a few runs or builds at any given time. That leads to running it more times, which is done in periods by the Benchmark Manager.
		</para>

		<para>
		The Benchmark Manager attempts to schedule analysis generator in periods set in analysis configuration. To adhere to some kind of load management, the analysis will not be scheduled if the analysis is still running. In that case the Benchmark Manager waits untill the analysis is finished and will schedule it right after that.
		</para>

		<para>
		Each generator run gets its own context. All the tasks in that context are considered part of the current analysis run.
		</para>
	</sect2>

</sect1>
