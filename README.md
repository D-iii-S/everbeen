# TODO

* Write README.md (used on jenkins build page and github project main page)

* Benchmark Manager
    * Storage information (in which db should results be stored, etc.)
    * Solve "error reporting" (onContextFailed event) for Benchmarks
    * If resubmitting a benchmark too often (more than x times in an hour), don't try
      any more
    * When resubmitting a benchmark, what should the behavior be? Should we wait for
      running contexts to finish?
* Results & Storage
	* Retrieving results from a user task
	* Allow a special user task (evaluator) to return a "file" (ZIP, PNG, CSV, ...)
* Context Planning & Dependencies
	* "Run a task after another has finished"
	* Load balancing - when we have 2 nodes, running "task test" benchmark puts
	  all contexts and tasks on the first runtime
	* Currently, we have "submit context, wait for it to finish" behavior. What other
	  cases should we have? Context queues? Running up to X context simultaneously?
* Software Repository
	* Implement "SNAPSHOT" versioning (don't cache)
	* Implement "empty cache" feature
* Web Interface
	* Edit properties when submitting (+ specify debug)
	* When Hazelcast gets disconnect, show an error to the user and on next page
	  redirect to the login page again
	* \*.\*
* `find . -type f | xargs grep TODO`


* Win32 support
* Native task API (task, benchmark)


# Wish-list
* Allow persistence & retrieval of "big files", or "lots of records" generated by user tasks
