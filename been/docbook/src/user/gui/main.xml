<?xml version="1.0" encoding="UTF-8"?>

<chapter
	xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xml="http://www.w3.org/XML/1998/namespace"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xsi:schemaLocation="http://docbook.org/ns/docbook ../../../docbook-5.0/xsd/docbook.xsd"
>
	<title>Web Inteface</title>

	<sect1>
		<title>Creating an analysis</title>

		<para>
		The Benchmarks tab contains list of analyses and link to create new analysis. The first screen in the process contains following field groups:

		<itemizedlist>
		    <listitem>
		    	<para>Analysis parameters</para>
		    	<para>
		    	<guilabel>Analysis name</guilabel> is mandatory field. It has to be unique java identifier. That means a single word that starts with a letter and contains only letters, digits and underscores. <guilabel>Description</guilabel> is optional field. It may be any text. <guilabel>Link to results web</guilabel> is also optional field. It may contain any text but only URL makes sense. Link to the Results Repository will be provided in the web interface if this field is left empty.
				</para>
			</listitem>
		    <listitem>
		    	<para>Generator Settings</para>
		    	<para>
				Web interface offers all the generator pluggable modules that the Benchmark Manager could find. You just have to pick the one you want. Generator module can't be changed after the analysis is saved. In the <guilabel>Generator host</guilabel> you need to enter the RSL for host where the <glossterm>generator-runner</glossterm> will be ran. Both fields are mandatory.
				</para>
			</listitem>
		    <listitem>
		    	<para>Evaluators</para>
		    	<para>
		    	In this section you can chose zero or more evaluators from the list. Evaluators may even be selected multiple times. They will be configured after the generator module.
				</para>
			</listitem>
		    <listitem>
		    	<para>Scheduling options</para>
		    	<para>
		    	If you want to run the analysis automatically you have to check the checkbox and enter the scheduling period time. Generator will be planned once the last generated tasks finish or after the given period expires, whichever comes later.
				</para>
			</listitem>
		</itemizedlist>

		Hit the <guibutton>next</guibutton> button after you enter all the required information and you will see the generator settings page. When you fill in the generator settings you will see one settigns page for each evaluator you have chosen on the first page. The last page is conventional overview of your settings with one tab per configuration screen. Analysis is saved only after you hit <guibutton>Finish</guibutton> on the overview page.
		</para>
	</sect1>

	<sect1>
		<title>Analyses progress and overview</title>

		<para>
		The information about analyses in web interface is displayed on three tabs. Contexts, tasks and overall status of a running analysis is displayed on <guilabel>Tasks</guilabel> tab. Analysis settings and status overview i displayed in <guilabel>Benchmarks</guilabel> tab. There you can also edit an analysis. The raw analysis output is displayed in the <guilabel>Results Repository</guilabel> tab.
		</para>


		<sect2>
			<title>Tasks tab</title>

			<sect3>
				<title>Contexts</title>

				<para>
				I the contexts overview you see some contexts related to your analysis. Let's say you created analysis named A. If the analysis has ran at least once then there should be at least one context named A-x-y where x is generator run number and y stands for number of failed generator runs in the respective generator run. First number is increased and second reset to 0 every time the generator succeeds in generating the task sequence. When generator fails, only the second number is increased. There are various reasons for generator failure - a bug in the generator, failure to comunicate with other parts of been, failure to get version list from a repository (svn, cvs, ...) and others.
				</para>

				<para>
				The number of contexts you see in the overview may vary. Ideally you should se at most one running context from each analysis and a fixed number of closed contexts (set in configuration). If there is more contexts then some of them have to be finished and open which means they contain some failures.
				</para>
			</sect3>

			<sect3>
				<title>Tasks</title>

				<para>
				Each context contains <glossterm>generator-runner</glossterm>, <glossterm>context-monitor</glossterm> and tasks generated by the generator. You can check status and parameters of each task. The columns in the task overview are self explanatory. The <guilabel>Task name</guilabel> coresponds to the task package name from the <guilabel>Packages</guilabel> tab. The context overview also contains link to the analysis overview page in the Benchmarks tab.
				</para>

				<para>
				The <glossterm>generator-runner</glossterm> task (or simply generator) is the first and the only one created by the Benchmark Manager. The rest of the tasks is created and scheduled by the generator. Once the generated tasks are scheduled, the generator reports end of his work to the Benchmark Manager and ends. The Benchmark Manager saves the time of the report as the last run and increases the run number in the analysis.
				</para>

				<para>
				The <glossterm>context-monitor</glossterm> task is scheduled by the generator even before the generating begins. The task starts as the second one and ends as the last in the context. It periodically monitors the other tasks in the context and waits for the context to be "finished". Context is considered finished when all tasks have finished (either succeded, failed or have been aborted). Additionally the <glossterm>context-monitor</glossterm> can check dependencies of scheduled tasks that have not started, yet. In that case the <glossterm>context-monitor</glossterm> can determine whether the dependencies can be met and treats the task as finished if it can't even start.
				</para>
			</sect3>

			<sect3 xml:id="been.user.gui.analyses.tasks.tree">
				<title>Tree view</title>

				<para>
				The tree view shows all the tasks sorted by the tree address. It's generator's responsibility to create a meaningful tree addresses for generated tasks. By default the analysis tasks are sorted by analysis name and context name. It's important to note that in the tree view there are all the tasks that have been scheduled since the last Task Manager start. In practice it means that clicking on a task in the tree view may lead to an error page bacause logs of that task have already been deleted from the Task Manager.
				</para>
			</sect3>
		</sect2>

		<sect2>
			<title>Benchmarks tab</title>

			<para>
			In this tab you see overview of all the analyses registered in the Benchmark Manager. Clicking on an analysis name opens the analysis settings overview. On this page you can review settings of the analysis and it is modules. On the analysis overview page there are three fields generated automatically by the Benchmark Manager. They are <guilabel>Status</guilabel>, <guilabel>Active contexts</guilabel> and <guilabel>Last run</guilabel>. <guilabel>Last run</guilabel> marks the time of last generator success. Status depends only on Active contexts which are kept in the Benchmark Manager.
			</para>

			<para>
			The list of active contexts should reflect the truth. Of course differences may occur and are generally caused by failures in the <glossterm>generator-runner</glossterm> task or in the <glossterm>context-monitor</glossterm> task. You can mark any listed context as finished. This is always necessary in case of inconsistency, or if you kill the <glossterm>context-monitor</glossterm> task in active context and don't want to start it manualy. You may also want to simply enable automatic analysis scheduling if you know the current <glossterm>analysis run</glossterm> will take much longer than usualy. Analysis won't be started while there is at least one active context.
			</para>
		</sect2>

		<sect2>
			<title>Results Repository tab</title>

			<para>
			The results repository shows analysis datasets with raw results data. If the Results Repository was a relational database then we could think about datasets as tables and analyses as separate databases. (<xref linkend="been.user.benchmark.results"/>) On the overview page you see list of all analyses that the repository knows about. This list doesn't have to corespond to the list of analyses in the Benchmark Manager. There may be some analyses missing if an analysis doesn't create its' own datasets and there may be some excessive analyses. For example the Xampler generators are able to share downloaded sources and builded binaries. They use additional analysis in the Results Repository for that purpose.
			</para>

			<para>
			In the analysis detail page you see one tab per dataset. Datasets have indexed key tags and not indexed regular tags. You can think about tags as table columns if you want. Each entry in the dataset has serial number. Serials denote the order of data insert. You can see the dataset data on each tab or you can monitor it in a separate window.
			</para>
		</sect2>
	</sect1>

</chapter>
