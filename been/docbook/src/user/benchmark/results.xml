<?xml version="1.0" encoding="UTF-8"?>

<sect1
	xml:id = "been.user.benchmark.results"
	xmlns="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xml="http://www.w3.org/XML/1998/namespace"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:xi="http://www.w3.org/2001/XInclude"
	xsi:schemaLocation="http://docbook.org/ns/docbook ../../../docbook-5.0/xsd/docbook.xsd"
>
	<title>Results Repository and Results</title>

	<para>In BEEN all the results of tasks are stored in one central database called the Results Repository. There are many types of results. Basically any task output can be considered its result. Download tasks produce the downloaded source files as their result,  build tasks produce the binaries, measuring tasks produce the measurements.</para>

	<para>
	</para>

	<sect2>
		<title>Results Repository service</title>

		<para>The Results Repository service serves as a central point for all tasks to store their results regardless of where they were running. That allows easy location and reuse of the results. Some tasks have dependencies and hosts set to allow data sharing. Central repository approach allows to share data between tasks that run on different hosts.</para>

		<para>The Results Repository has database-like structure. It has knowledge about analyses, each analysis contains some datasets and each dataset is a repository for tagged data. You can think about the data and their tags as about one row in conventional database model, each tag then represents one column. The datasets are then tables and analyses are databases.</para>

		<para>Different tags can have different values types of course. To unify this, the Results Repository works with abstract DataHandle type which can hold values of different types. When sending data to the Results Repository, a task therefore sends only list of key-DataHandle mappings. The correct data types are checked on the Results Repository side.
		</para>

		<para>When a correct DataHandle tuple arives at the repository, it gets unique serial number and is saved in the appropriate dataset. The serial numbers hold information about the order of data arival. Ussually it's important only within one dataset. With this field, evaluators can check which data are new and which are old.
		</para>

	</sect2>

	<sect2>
		<title>Evaluators and triggers</title>

		<para>Results Repository and evaluator tasks are tightly connected through the Results Repository trigger mechanism. Each dataset can have unlimited number of triggers. These triggers have condition and a BEEN task as their parameters. For every new data ariving to the dataset the condition is checked. If it evaluates to true then the task is scheduled. We sometimes refer to this task as a triggered task.
		</para>

		<para>Of course this could mean that the task is triggered as often as the data are coming to the Results Repository and it may potentially overload the Task Manager. For that purpose every trigger is disabled right after the task is scheduled. The Evaluator task then has the responsibility to report back to the Results Repository that it has finished. If there were any data that would fire the trigger while the task was running, then the task is immediately scheduled again.
		</para>

		<para>The triggered task receives a few parameters generated automatically by the Results Repository. They most important ones are analysis name, dataset name and a data serial number. Scheduling only one task at a time creates the possibility that one task is triggered by several data rows. Therefore the serial given as the parameter to the trigger means the first data that haven't been processed by the previously scheduled task. When the evaluator task finishes it doesn't only report its own success but also tells Results Repository what is the highest data serial it has processed.
		</para>

		<para>This mechanism makes evaluating the data an event-driven process. Evaluator tasks are scheduled whenever new data appear in the repository so even "infinite" (i.e. analyses, that never terminate) analyses are easily evaluated. Evaluator tasks can save their results back to the Results Repository. Therefore they can be easily concatenated to create an evaluation tree or directed acyclic graph.</para>

	</sect2>

</sect1>
